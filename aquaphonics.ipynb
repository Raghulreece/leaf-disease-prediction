{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeeVJmSjaacz",
        "outputId": "4c068eba-10a7-4e45-d82f-c1b62ee6338e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7Y0pzHJyrh3"
      },
      "outputs": [],
      "source": [
        "!unzip drive/MyDrive/aquaphonics/dataset.zip -d ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTjIT46zzOpm"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import h5py\n",
        "import os\n",
        "\n",
        "class HDF5DatasetWriter:\n",
        "\tdef __init__(self, dims, outputPath, dataKey=\"images\",\n",
        "\t\tbufSize=1000):\n",
        "\t\t# check to see if the output path exists, and if so, raise\n",
        "\t\t# an exception\n",
        "\t\tif os.path.exists(outputPath):\n",
        "\t\t\traise ValueError(\"The supplied `outputPath` already \"\n",
        "\t\t\t\t\"exists and cannot be overwritten. Manually delete \"\n",
        "\t\t\t\t\"the file before continuing.\", outputPath)\n",
        "\n",
        "\t\t# open the HDF5 database for writing and create two datasets:\n",
        "\t\t# one to store the images/features and another to store the\n",
        "\t\t# class labels\n",
        "\t\tself.db = h5py.File(outputPath, \"w\")\n",
        "\t\tself.data = self.db.create_dataset(dataKey, dims,\n",
        "\t\t\tdtype=\"float\")\n",
        "\t\tself.labels = self.db.create_dataset(\"labels\", (dims[0],),\n",
        "\t\t\tdtype=\"int\")\n",
        "\n",
        "\t\t# store the buffer size, then initialize the buffer itself\n",
        "\t\t# along with the index into the datasets\n",
        "\t\tself.bufSize = bufSize\n",
        "\t\tself.buffer = {\"data\": [], \"labels\": []}\n",
        "\t\tself.idx = 0\n",
        "\n",
        "\tdef add(self, rows, labels):\n",
        "\t\t# add the rows and labels to the buffer\n",
        "\t\tself.buffer[\"data\"].extend(rows)\n",
        "\t\tself.buffer[\"labels\"].extend(labels)\n",
        "\n",
        "\t\t# check to see if the buffer needs to be flushed to disk\n",
        "\t\tif len(self.buffer[\"data\"]) >= self.bufSize:\n",
        "\t\t\tself.flush()\n",
        "\n",
        "\tdef flush(self):\n",
        "\t\t# write the buffers to disk then reset the buffer\n",
        "\t\ti = self.idx + len(self.buffer[\"data\"])\n",
        "\t\tself.data[self.idx:i] = self.buffer[\"data\"]\n",
        "\t\tself.labels[self.idx:i] = self.buffer[\"labels\"]\n",
        "\t\tself.idx = i\n",
        "\t\tself.buffer = {\"data\": [], \"labels\": []}\n",
        "\n",
        "\tdef storeClassLabels(self, classLabels):\n",
        "\t\t# create a dataset to store the actual class label names,\n",
        "\t\t# then store the class labels\n",
        "\t\tdt = h5py.special_dtype(vlen=str) # `vlen=unicode` for Py2.7\n",
        "\t\tlabelSet = self.db.create_dataset(\"label_names\",\n",
        "\t\t\t(len(classLabels),), dtype=dt)\n",
        "\t\tlabelSet[:] = classLabels\n",
        "\n",
        "\tdef close(self):\n",
        "\t\t# check to see if there are any other entries in the buffer\n",
        "\t\t# that need to be flushed to disk\n",
        "\t\tif len(self.buffer[\"data\"]) > 0:\n",
        "\t\t\tself.flush()\n",
        "\n",
        "\t\t# close the dataset\n",
        "\t\tself.db.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK4D0ff3-6AY"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZf_5579zIHH"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from keras.applications import imagenet_utils\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import random\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae44wBsPzz9-",
        "outputId": "f7834bda-d0ca-4833-d153-800a8a8fcdb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] loading network...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Features: 100% |####################################| Time:  0:00:18\n"
          ]
        }
      ],
      "source": [
        "# store the batch size in a convenience variable\n",
        "bs = 16\n",
        "\n",
        "# grab the list of images that we'll be describing then randomly\n",
        "# shuffle them to allow for easy training and testing splits via\n",
        "# array slicing during training time\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(\"dataset\"))\n",
        "random.shuffle(imagePaths)\n",
        "\n",
        "# extract the class labels from the image paths then encode the\n",
        "# labels\n",
        "labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "\n",
        "# load the VGG16 network\n",
        "print(\"[INFO] loading network...\")\n",
        "model = VGG16(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "# initialize the HDF5 dataset writer, then store the class label\n",
        "# names in the dataset\n",
        "dataset = HDF5DatasetWriter((len(imagePaths), 512 * 7 * 7),\n",
        "\t\"features.hdf5\", dataKey=\"features\", bufSize=1000)\n",
        "dataset.storeClassLabels(le.classes_)\n",
        "\n",
        "# initialize the progress bar\n",
        "widgets = [\"Extracting Features: \", progressbar.Percentage(), \" \",\n",
        "\tprogressbar.Bar(), \" \", progressbar.ETA()]\n",
        "pbar = progressbar.ProgressBar(maxval=len(imagePaths),\n",
        "\twidgets=widgets).start()\n",
        "\n",
        "# loop over the images in patches\n",
        "for i in np.arange(0, len(imagePaths), bs):\n",
        "\t# extract the batch of images and labels, then initialize the\n",
        "\t# list of actual images that will be passed through the network\n",
        "\t# for feature extraction\n",
        "\tbatchPaths = imagePaths[i:i + bs]\n",
        "\tbatchLabels = labels[i:i + bs]\n",
        "\tbatchImages = []\n",
        "\n",
        "\t# loop over the images and labels in the current batch\n",
        "\tfor (j, imagePath) in enumerate(batchPaths):\n",
        "\t\t# load the input image using the Keras helper utility\n",
        "\t\t# while ensuring the image is resized to 224x224 pixels\n",
        "\t\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\t\timage = img_to_array(image)\n",
        "\n",
        "\t\t# preprocess the image by (1) expanding the dimensions and\n",
        "\t\t# (2) subtracting the mean RGB pixel intensity from the\n",
        "\t\t# ImageNet dataset\n",
        "\t\timage = np.expand_dims(image, axis=0)\n",
        "\t\timage = imagenet_utils.preprocess_input(image)\n",
        "\n",
        "\t\t# add the image to the batch\n",
        "\t\tbatchImages.append(image)\n",
        "\n",
        "\t# pass the images through the network and use the outputs as\n",
        "\t# our actual features\n",
        "\tbatchImages = np.vstack(batchImages)\n",
        "\tfeatures = model.predict(batchImages, batch_size=bs)\n",
        "\n",
        "\t# reshape the features so that each image is represented by\n",
        "\t# a flattened feature vector of the `MaxPooling2D` outputs\n",
        "\tfeatures = features.reshape((features.shape[0], 512 * 7 * 7))\n",
        "\n",
        "\t# add the features and labels to our HDF5 dataset\n",
        "\tdataset.add(features, batchLabels)\n",
        "\tpbar.update(i)\n",
        "\n",
        "# close the dataset\n",
        "dataset.close()\n",
        "pbar.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4JXWYu27IgS"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle\n",
        "import h5py\n",
        "\n",
        "# open the HDF5 database for reading then determine the index of\n",
        "# the training and testing split, provided that this data was\n",
        "# already shuffled *prior* to writing it to disk\n",
        "db = h5py.File('features.hdf5', \"r\")\n",
        "i = int(db[\"labels\"].shape[0] * 0.75)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqQNXXGD7VOQ"
      },
      "source": [
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFvLJRod7sWd"
      },
      "outputs": [],
      "source": [
        "classNames = ['Bacterial_spot', 'Early_blight', 'Late_blight', 'Leaf_Mold', 'Septoria_leaf_spot',\n",
        "              'Spider_mites Two-spotted_spider_mite', 'Target_Spot', 'Tomato_Yellow_Leaf_Curl_Virus',\n",
        "              'healthy', 'mosaic_virus']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUC_0rLe7UjC",
        "outputId": "52019eb9-000f-4350-a94e-11a3f93615af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] evaluating Decision_Tree classifier...\n",
            "[INFO] evaluating...\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                      Bacterial_spot       0.36      0.43      0.39        21\n",
            "                        Early_blight       0.62      0.55      0.58        33\n",
            "                         Late_blight       0.65      0.43      0.52        30\n",
            "                           Leaf_Mold       0.76      0.53      0.63        30\n",
            "                  Septoria_leaf_spot       0.56      0.79      0.65        19\n",
            "Spider_mites Two-spotted_spider_mite       0.44      0.47      0.46        17\n",
            "                         Target_Spot       0.42      0.59      0.49        17\n",
            "       Tomato_Yellow_Leaf_Curl_Virus       0.90      0.83      0.86        23\n",
            "                             healthy       0.64      0.72      0.68        25\n",
            "                        mosaic_virus       0.59      0.63      0.61        35\n",
            "\n",
            "                            accuracy                           0.59       250\n",
            "                           macro avg       0.60      0.60      0.59       250\n",
            "                        weighted avg       0.61      0.59      0.59       250\n",
            "\n",
            "0.6479225000351762\n",
            "[INFO] saving model...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# train and evaluate decision_tree classifier on the raw pixel intensities\n",
        "print(\"[INFO] evaluating Decision_Tree classifier...\")\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(db[\"features\"][:i], db[\"labels\"][:i])\n",
        "\n",
        "# evaluate the model\n",
        "print(\"[INFO] evaluating...\")\n",
        "preds = model.predict(db[\"features\"][i:])\n",
        "print(classification_report(db[\"labels\"][i:], preds,\n",
        "\ttarget_names=classNames))\n",
        "\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving model...\")\n",
        "f = open('decision_tree.p', \"wb\")\n",
        "f.write(pickle.dumps(model))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vmx0GSBJ_CuH"
      },
      "source": [
        "### K-Nearest Neighbour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaWwZ6UV98bi",
        "outputId": "1e6d7d14-9dcd-4932-b1e3-5133e5526611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] evaluating k-NN classifier...\n",
            "[INFO] evaluating...\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                      Bacterial_spot       1.00      0.86      0.92        21\n",
            "                        Early_blight       0.91      0.91      0.91        33\n",
            "                         Late_blight       1.00      0.90      0.95        30\n",
            "                           Leaf_Mold       1.00      0.87      0.93        30\n",
            "                  Septoria_leaf_spot       1.00      0.89      0.94        19\n",
            "Spider_mites Two-spotted_spider_mite       0.93      0.76      0.84        17\n",
            "                         Target_Spot       1.00      0.59      0.74        17\n",
            "       Tomato_Yellow_Leaf_Curl_Virus       1.00      0.91      0.95        23\n",
            "                             healthy       0.48      1.00      0.65        25\n",
            "                        mosaic_virus       1.00      0.91      0.96        35\n",
            "\n",
            "                            accuracy                           0.88       250\n",
            "                           macro avg       0.93      0.86      0.88       250\n",
            "                        weighted avg       0.93      0.88      0.89       250\n",
            "\n",
            "0.8759831717578196\n",
            "[INFO] saving model...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# train and evaluate a k-NN classifier on the raw pixel intensities\n",
        "print(\"[INFO] evaluating k-NN classifier...\")\n",
        "model = KNeighborsClassifier(n_neighbors=1,\n",
        "\tn_jobs=1)\n",
        "model.fit(db[\"features\"][:i], db[\"labels\"][:i])\n",
        "\n",
        "# evaluate the model\n",
        "print(\"[INFO] evaluating...\")\n",
        "preds = model.predict(db[\"features\"][i:])\n",
        "print(classification_report(db[\"labels\"][i:], preds,\n",
        "\ttarget_names=classNames))\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving model...\")\n",
        "f = open('knn.p', \"wb\")\n",
        "f.write(pickle.dumps(model))\n",
        "f.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r56reobO_Ltg"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzRlgw3k9QZZ",
        "outputId": "668fcb1f-d37c-419b-b1f1-da19bdf28914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] evaluating Random_Forest classifier...\n",
            "[INFO] evaluating...\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                      Bacterial_spot       0.83      0.95      0.89        21\n",
            "                        Early_blight       1.00      0.85      0.92        33\n",
            "                         Late_blight       0.94      1.00      0.97        30\n",
            "                           Leaf_Mold       1.00      0.93      0.97        30\n",
            "                  Septoria_leaf_spot       0.95      1.00      0.97        19\n",
            "Spider_mites Two-spotted_spider_mite       1.00      0.94      0.97        17\n",
            "                         Target_Spot       0.89      1.00      0.94        17\n",
            "       Tomato_Yellow_Leaf_Curl_Virus       1.00      1.00      1.00        23\n",
            "                             healthy       1.00      1.00      1.00        25\n",
            "                        mosaic_virus       1.00      1.00      1.00        35\n",
            "\n",
            "                            accuracy                           0.96       250\n",
            "                           macro avg       0.96      0.97      0.96       250\n",
            "                        weighted avg       0.97      0.96      0.96       250\n",
            "\n",
            "0.9710149005923654\n",
            "[INFO] saving model...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# train and evaluate random_forest classifier on the raw pixel intensities\n",
        "print(\"[INFO] evaluating Random_Forest classifier...\")\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(db[\"features\"][:i], db[\"labels\"][:i])\n",
        "\n",
        "# evaluate the model\n",
        "print(\"[INFO] evaluating...\")\n",
        "preds = model.predict(db[\"features\"][i:])\n",
        "print(classification_report(db[\"labels\"][i:], preds,\n",
        "\ttarget_names=classNames))\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving model...\")\n",
        "f = open('random_forest.p', \"wb\")\n",
        "f.write(pickle.dumps(model))\n",
        "f.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXVk9Mgk_OfY"
      },
      "source": [
        "### Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awnxk-h_9p2R",
        "outputId": "2b03d665-024d-4a60-efa0-bf54ca39edc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] evaluating SVM classifier...\n",
            "[INFO] evaluating...\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "                      Bacterial_spot       1.00      1.00      1.00        21\n",
            "                        Early_blight       1.00      1.00      1.00        33\n",
            "                         Late_blight       1.00      1.00      1.00        30\n",
            "                           Leaf_Mold       1.00      0.97      0.98        30\n",
            "                  Septoria_leaf_spot       1.00      1.00      1.00        19\n",
            "Spider_mites Two-spotted_spider_mite       1.00      1.00      1.00        17\n",
            "                         Target_Spot       0.94      1.00      0.97        17\n",
            "       Tomato_Yellow_Leaf_Curl_Virus       1.00      1.00      1.00        23\n",
            "                             healthy       1.00      1.00      1.00        25\n",
            "                        mosaic_virus       1.00      1.00      1.00        35\n",
            "\n",
            "                            accuracy                           1.00       250\n",
            "                           macro avg       0.99      1.00      1.00       250\n",
            "                        weighted avg       1.00      1.00      1.00       250\n",
            "\n",
            "[INFO] saving model...\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "# train and evaluate svm classifier on the raw pixel intensities\n",
        "print(\"[INFO] evaluating SVM classifier...\")\n",
        "model = svm.SVC(kernel='linear')\n",
        "model.fit(db[\"features\"][:i], db[\"labels\"][:i])\n",
        "\n",
        "# evaluate the model\n",
        "print(\"[INFO] evaluating...\")\n",
        "preds = model.predict(db[\"features\"][i:])\n",
        "print(classification_report(db[\"labels\"][i:], preds,\n",
        "\ttarget_names=classNames))\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving model...\")\n",
        "f = open('svm.p', \"wb\")\n",
        "f.write(pickle.dumps(model))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXRcwGkD_UCY"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuiN3vNwccyo",
        "outputId": "98360cac-8163-4b64-e32e-18db59bd40b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=86ba9867d10f16675bbc2ae53cbfe56980406395c1247afccb50e0654f4466c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2022.5.18.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask-cors==3.0.7\n",
            "  Downloading Flask_Cors-3.0.7-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors==3.0.7) (1.15.0)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.7/dist-packages (from flask-cors==3.0.7) (1.1.4)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors==3.0.7) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors==3.0.7) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors==3.0.7) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors==3.0.7) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.9->flask-cors==3.0.7) (2.0.1)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-3.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n",
        "!pip install flask-ngrok\n",
        "!pip install flask-cors==3.0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7Ge6IfWcf-Q",
        "outputId": "2773e264-812e-49f7-99dc-a843552f096d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYih6-oBchjo",
        "outputId": "d02bdb8b-373b-4622-a5a1-4903ed76578d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://eaf7-34-80-91-189.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jun/2022 04:53:40] \"\u001b[37mOPTIONS /aqua HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading model...\n",
            "[INFO] loading image...\n",
            "[INFO] loading network...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jun/2022 04:53:42] \"\u001b[37mPOST /aqua HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading pre-trained network...\n",
            "[INFO] predicting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jun/2022 04:53:52] \"\u001b[37mOPTIONS /aqua HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading model...\n",
            "[INFO] loading image...\n",
            "[INFO] loading network...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [02/Jun/2022 04:53:53] \"\u001b[37mPOST /aqua HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading pre-trained network...\n",
            "[INFO] predicting...\n"
          ]
        }
      ],
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask\n",
        "from flask import Flask, app, request\n",
        "import json\n",
        "from base64 import b64decode, b64encode\n",
        "from flask_cors import CORS, cross_origin\n",
        "\n",
        "from keras.applications import imagenet_utils\n",
        "from keras.preprocessing.image import img_to_array\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras.models import load_model\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imread, imshow\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "app = Flask(__name__)\n",
        "cors = CORS(app)\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run\n",
        "@app.route('/aqua', methods=['GET','POST'])\n",
        "# @cross_origin()\n",
        "def login():\n",
        "\n",
        "    result = input(request.json['uri'])\n",
        "    return result\n",
        "\n",
        "def input(uri):\n",
        "\n",
        "    print(\"[INFO] loading model...\")\n",
        "    header, encoded = uri.split(\",\", 1)\n",
        "    data = b64decode(encoded)\n",
        "    f = open(\"uriimage.jpg\", \"wb\")\n",
        "    f.write(data)\n",
        "    # grab the image\n",
        "    orig = cv2.imread('uriimage.jpg')\n",
        "    imagePath = orig.copy()\n",
        "\n",
        "    # load the VGG16 network\n",
        "    model = VGG16(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "    # image is resized to 224x224 pixels\n",
        "    image  =cv2.resize(imagePath, (224, 224))\n",
        "    image = img_to_array(image)\n",
        "\n",
        "    # preprocess the image by (1) expanding the dimensions and\n",
        "    # (2) subtracting the mean RGB pixel intensity from the\n",
        "    # ImageNet dataset\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = imagenet_utils.preprocess_input(image)\n",
        "\n",
        "    # pass the images through the network and use the outputs as\n",
        "    # our actual features\n",
        "    features = model.predict(image)\n",
        "\n",
        "    # reshape the features so that each image is represented by\n",
        "    # a flattened feature vector of the `MaxPooling2D` outputs\n",
        "    features = features.reshape((features.shape[0], 512 * 7 * 7))\n",
        "\n",
        "    # load the trained network\n",
        "    loaded_model = pickle.load(open(\"svm.p\", 'rb'))\n",
        "\n",
        "    # make predictions on the images\n",
        "    preds = loaded_model.predict(features)\n",
        "    \n",
        "    return ({\"data\":str(classNames[preds[0]])})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "aquaphonics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
